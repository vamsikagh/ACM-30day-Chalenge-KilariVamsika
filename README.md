# ACM 30-Day Machine Learning Challenge ‚Äì Kilari Vamsika

Hi! I'm Vamsika, a data science enthusiast eager to solve real-world problems using ML, statistics, and analytical tools. This repository is a curated journey through multiple challenge cycles focusing on preprocessing, modeling, and evaluation tasks to build solid data intuition and technical fluency.

---

## Cycle 1

### üìÖ Daily Progress

| Day | Task Summary |
|-----|--------------|
| **Day 1** | Performed basic data cleaning and exploratory data analysis (EDA) on a burnout dataset. Handled missing values, explored distributions, and generated summary statistics. |
| **Day 2** | Applied preprocessing techniques for machine learning. Encoded categorical variables using OneHotEncoder and combined them with scaled numerical features to prepare the dataset for modeling. Performed Regression and decided which is best among three the types. |
| **Day 3** | Preprocessed data using one-hot encoding and scaling. Trained Logistic Regression and LDA models to classify burnout, then evaluated them using Accuracy, Confusion Matrix, and ROC-AUC with ROC curve visualization. |
| **Day 4** | Trained Decision Tree, Random Forest, and k-NN models. Used Mutual Information to pick top 3 features and compared model accuracy before and after feature selection. |
| **Day 5** | A Random Forest model predicts burnout risk after encoding and scaling the data. The top 3 features were used to build a simpler model without losing accuracy. A heatmap shows how these features relate to each other. |
| **Main Challenge** | This model predicts medical insurance costs after cleaning and preprocessing the data. Tested different regression methods to find the best fit. Using mutual info, we picked the most important features and trained a Random Forest model for better accuracy. The final model gives reliable predictions with a solid R¬≤ score. It's efficient yet powerful enough for real-world use. |

### üìÅ Repository Contents ‚Äì Cycle 1
- `Day1.ipynb` ‚Äì Data cleaning and exploration
- `Day2.ipynb` ‚Äì Feature encoding and preprocessing
- `Day3.ipynb` ‚Äì Classifier Arena
- `Day4.ipynb` ‚Äì Tree-Based Models + k-NN + Feature Selection
- `Day5.ipynb` ‚Äì 3-Feature Showdown
- `MAIN CHALLENGE.ipynb` ‚Äì Medical cost regression project

---

## Cycle 2

### üìÖ Daily Progress

| Day | Task Summary |
|-----|--------------|
| **Day 6** | Explored ensemble learning with Bagging and Boosting using the Breast Cancer dataset. Trained and compared Random Forest, AdaBoost, and XGBoost classifiers. Accuracy was measured for all models and the best-performing one was highlighted. Label encoding and feature scaling were applied, followed by model evaluation using confusion matrix and classification report. |

---

### üìÅ Repository Contents ‚Äì Cycle 2
- `Day6.ipynb` ‚Äì Bagging vs Boosting (Random Forest, AdaBoost, XGBoost)

---

## üß∞ Tools & Libraries Used
- Python
- Pandas
- NumPy
- Scikit-learn
- XGBoost
- Matplotlib / Seaborn

